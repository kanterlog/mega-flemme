"""
üß† Service IA Hybride - GPT + Gemini pour Sylvie
Phase 3.6 - Intelligence artificielle multi-mod√®les

Service intelligent qui combine OpenAI GPT et Google Gemini
pour une exp√©rience utilisateur optimale
"""

import asyncio
from enum import Enum
from typing import Dict, List, Optional, Any, Tuple
import structlog
import json
from datetime import datetime
import google.generativeai as genai
from openai import OpenAI

from app.utils.config import settings

logger = structlog.get_logger(__name__)

class AIModel(str, Enum):
    """Mod√®les IA disponibles"""
    OPENAI = "openai"
    GEMINI = "gemini"

class AIStrategy(str, Enum):
    """Strat√©gies d'utilisation IA"""
    HYBRID = "hybrid"          # Utilise les deux mod√®les intelligemment
    OPENAI_ONLY = "openai"     # OpenAI uniquement
    GEMINI_ONLY = "gemini"     # Gemini uniquement
    FALLBACK = "fallback"      # Un mod√®le principal + fallback

class TaskType(str, Enum):
    """Types de t√¢ches pour l'optimisation mod√®le"""
    CONVERSATION = "conversation"
    INTENT_ANALYSIS = "intent_analysis"
    CODE_GENERATION = "code_generation"
    DATA_ANALYSIS = "data_analysis"
    CREATIVE_WRITING = "creative_writing"
    TECHNICAL_SUPPORT = "technical_support"

class AIResponse:
    """R√©ponse unifi√©e des mod√®les IA"""
    def __init__(self, content: str, model: AIModel, metadata: Dict[str, Any] = None):
        self.content = content
        self.model = model
        self.metadata = metadata or {}
        self.timestamp = datetime.utcnow()

class HybridAIService:
    """Service IA hybride intelligent"""
    
    def __init__(self):
        self.openai_client = None
        self.gemini_model = None
        self.strategy = AIStrategy(settings.AI_MODEL_STRATEGY)
        
        # Initialisation des clients IA
        self._initialize_clients()
        
        # Configuration des sp√©cialisations par t√¢che
        self.task_specialization = {
            TaskType.CONVERSATION: AIModel.OPENAI,      # GPT excelle en conversation
            TaskType.INTENT_ANALYSIS: AIModel.GEMINI,   # Gemini bon en analyse
            TaskType.CODE_GENERATION: AIModel.OPENAI,   # GPT meilleur en code
            TaskType.DATA_ANALYSIS: AIModel.GEMINI,     # Gemini efficace sur les donn√©es
            TaskType.CREATIVE_WRITING: AIModel.OPENAI,  # GPT plus cr√©atif
            TaskType.TECHNICAL_SUPPORT: AIModel.GEMINI  # Gemini factuel
        }
        
        logger.info("üß† Service IA hybride initialis√©", 
                   strategy=self.strategy,
                   openai_available=self.openai_client is not None,
                   gemini_available=self.gemini_model is not None)
    
    def _initialize_clients(self):
        """Initialisation des clients IA"""
        
        # Client OpenAI
        if settings.OPENAI_API_KEY and settings.OPENAI_API_KEY != "sk-your-openai-key-here":
            try:
                self.openai_client = OpenAI(api_key=settings.OPENAI_API_KEY)
                logger.info("‚úÖ Client OpenAI initialis√©")
            except Exception as e:
                logger.error("‚ùå Erreur initialisation OpenAI", error=str(e))
        
        # Client Gemini
        if settings.GOOGLE_AI_KEY and settings.GOOGLE_AI_KEY != "your-gemini-api-key-here":
            try:
                genai.configure(api_key=settings.GOOGLE_AI_KEY)
                self.gemini_model = genai.GenerativeModel(settings.GEMINI_MODEL)
                logger.info("‚úÖ Client Gemini initialis√©")
            except Exception as e:
                logger.error("‚ùå Erreur initialisation Gemini", error=str(e))
    
    async def generate_response(self, 
                              prompt: str, 
                              task_type: TaskType = TaskType.CONVERSATION,
                              max_tokens: int = 1000,
                              temperature: float = 0.7,
                              system_prompt: str = None) -> AIResponse:
        """
        G√©n√©ration de r√©ponse avec s√©lection intelligente du mod√®le
        
        Args:
            prompt: Prompt utilisateur
            task_type: Type de t√¢che pour optimiser le choix du mod√®le
            max_tokens: Limite de tokens
            temperature: Cr√©ativit√© (0-1)
            system_prompt: Prompt syst√®me optionnel
            
        Returns:
            R√©ponse IA unifi√©e
        """
        
        # S√©lection du mod√®le selon la strat√©gie
        selected_model = self._select_model(task_type)
        
        try:
            if selected_model == AIModel.OPENAI and self.openai_client:
                return await self._generate_openai_response(
                    prompt, max_tokens, temperature, system_prompt
                )
            elif selected_model == AIModel.GEMINI and self.gemini_model:
                return await self._generate_gemini_response(
                    prompt, max_tokens, temperature, system_prompt
                )
            else:
                # Fallback vers le mod√®le disponible
                return await self._generate_fallback_response(
                    prompt, max_tokens, temperature, system_prompt
                )
                
        except Exception as e:
            logger.error(f"‚ùå Erreur {selected_model}", error=str(e))
            
            # Tentative avec le mod√®le de fallback
            fallback_model = AIModel.GEMINI if selected_model == AIModel.OPENAI else AIModel.OPENAI
            
            try:
                if fallback_model == AIModel.OPENAI and self.openai_client:
                    return await self._generate_openai_response(
                        prompt, max_tokens, temperature, system_prompt
                    )
                elif fallback_model == AIModel.GEMINI and self.gemini_model:
                    return await self._generate_gemini_response(
                        prompt, max_tokens, temperature, system_prompt
                    )
            except Exception as fallback_error:
                logger.error("‚ùå Erreur fallback", error=str(fallback_error))
                
            # R√©ponse d'erreur gracieuse
            return AIResponse(
                content="üòÖ D√©sol√©e, j'ai un probl√®me technique temporaire. Pouvez-vous r√©essayer ?",
                model=selected_model,
                metadata={"error": str(e)}
            )
    
    def _select_model(self, task_type: TaskType) -> AIModel:
        """S√©lection intelligente du mod√®le selon la t√¢che et la strat√©gie"""
        
        if self.strategy == AIStrategy.OPENAI_ONLY:
            return AIModel.OPENAI
        elif self.strategy == AIStrategy.GEMINI_ONLY:
            return AIModel.GEMINI
        elif self.strategy == AIStrategy.HYBRID:
            # S√©lection bas√©e sur la sp√©cialisation de t√¢che
            return self.task_specialization.get(task_type, AIModel.OPENAI)
        elif self.strategy == AIStrategy.FALLBACK:
            # Mod√®le principal avec fallback
            primary = AIModel(settings.PRIMARY_MODEL)
            return primary
        
        return AIModel.OPENAI  # D√©faut
    
    async def _generate_openai_response(self, 
                                      prompt: str, 
                                      max_tokens: int, 
                                      temperature: float,
                                      system_prompt: str = None) -> AIResponse:
        """G√©n√©ration de r√©ponse via OpenAI GPT"""
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        response = self.openai_client.chat.completions.create(
            model=settings.OPENAI_MODEL,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        return AIResponse(
            content=response.choices[0].message.content,
            model=AIModel.OPENAI,
            metadata={
                "tokens_used": response.usage.total_tokens if response.usage else 0,
                "model": settings.OPENAI_MODEL
            }
        )
    
    async def _generate_gemini_response(self, 
                                      prompt: str, 
                                      max_tokens: int, 
                                      temperature: float,
                                      system_prompt: str = None) -> AIResponse:
        """G√©n√©ration de r√©ponse via Google Gemini"""
        
        # Construction du prompt avec syst√®me si fourni
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"{system_prompt}\n\nUtilisateur: {prompt}"
        
        # Configuration de g√©n√©ration
        generation_config = genai.types.GenerationConfig(
            max_output_tokens=max_tokens,
            temperature=temperature
        )
        
        response = self.gemini_model.generate_content(
            full_prompt,
            generation_config=generation_config
        )
        
        return AIResponse(
            content=response.text,
            model=AIModel.GEMINI,
            metadata={
                "model": settings.GEMINI_MODEL,
                "candidates_count": len(response.candidates) if hasattr(response, 'candidates') else 1
            }
        )
    
    async def _generate_fallback_response(self, 
                                        prompt: str, 
                                        max_tokens: int, 
                                        temperature: float,
                                        system_prompt: str = None) -> AIResponse:
        """G√©n√©ration avec le mod√®le disponible en fallback"""
        
        if self.openai_client:
            return await self._generate_openai_response(prompt, max_tokens, temperature, system_prompt)
        elif self.gemini_model:
            return await self._generate_gemini_response(prompt, max_tokens, temperature, system_prompt)
        else:
            raise Exception("Aucun mod√®le IA disponible")
    
    async def compare_responses(self, 
                              prompt: str, 
                              task_type: TaskType = TaskType.CONVERSATION) -> Dict[str, AIResponse]:
        """
        Compare les r√©ponses des deux mod√®les (pour debug/optimisation)
        
        Returns:
            Dictionnaire avec les r√©ponses des deux mod√®les
        """
        
        responses = {}
        
        # Test OpenAI
        if self.openai_client:
            try:
                responses["openai"] = await self._generate_openai_response(prompt, 500, 0.7)
            except Exception as e:
                logger.error("‚ùå Erreur comparaison OpenAI", error=str(e))
        
        # Test Gemini
        if self.gemini_model:
            try:
                responses["gemini"] = await self._generate_gemini_response(prompt, 500, 0.7)
            except Exception as e:
                logger.error("‚ùå Erreur comparaison Gemini", error=str(e))
        
        return responses
    
    def get_service_status(self) -> Dict[str, Any]:
        """√âtat du service IA hybride"""
        return {
            "strategy": self.strategy,
            "models_available": {
                "openai": self.openai_client is not None,
                "gemini": self.gemini_model is not None
            },
            "configuration": {
                "primary_model": settings.PRIMARY_MODEL,
                "fallback_model": settings.FALLBACK_MODEL,
                "openai_model": settings.OPENAI_MODEL,
                "gemini_model": settings.GEMINI_MODEL
            },
            "task_specialization": {k.value: v.value for k, v in self.task_specialization.items()}
        }

# Instance globale du service IA hybride
hybrid_ai = HybridAIService()
