"""
ðŸ§  Service IA Hybride - GPT + Gemini pour Sylvie
Phase 3.6 - Intelligence artificielle multi-modÃ¨les

Service intelligent qui combine OpenAI GPT et Google Gemini
pour une expÃ©rience utilisateur optimale
"""

import asyncio
from enum import Enum
from typing import Dict, List, Optional, Any, Tuple
import structlog
import json
from datetime import datetime
import google.generativeai as genai
from openai import OpenAI

from app.utils.config import settings

logger = structlog.get_logger(__name__)

class AIModel(str, Enum):
    """ModÃ¨les IA disponibles"""
    OPENAI = "openai"
    GEMINI = "gemini"

class AIStrategy(str, Enum):
    """StratÃ©gies d'utilisation IA"""
    HYBRID = "hybrid"          # Utilise les deux modÃ¨les intelligemment
    OPENAI_ONLY = "openai"     # OpenAI uniquement
    GEMINI_ONLY = "gemini"     # Gemini uniquement
    FALLBACK = "fallback"      # Un modÃ¨le principal + fallback

class TaskType(str, Enum):
    """Types de tÃ¢ches pour l'optimisation modÃ¨le"""
    CONVERSATION = "conversation"
    INTENT_ANALYSIS = "intent_analysis"
    CODE_GENERATION = "code_generation"
    DATA_ANALYSIS = "data_analysis"
    CREATIVE_WRITING = "creative_writing"
    TECHNICAL_SUPPORT = "technical_support"

class AIResponse:
    """RÃ©ponse unifiÃ©e des modÃ¨les IA"""
    def __init__(self, content: str, model: AIModel, metadata: Dict[str, Any] = None):
        self.content = content
        self.model = model
        self.metadata = metadata or {}
        self.timestamp = datetime.utcnow()

class HybridAIService:
    """Service IA hybride intelligent"""
    
    def __init__(self):
        self.openai_client = None
        self.gemini_model = None
        self.strategy = AIStrategy(settings.AI_MODEL_STRATEGY)
        
        # Initialisation des clients IA
        self._initialize_clients()
        
        # Configuration des spÃ©cialisations par tÃ¢che
        self.task_specialization = {
            TaskType.CONVERSATION: AIModel.OPENAI,      # GPT excelle en conversation
            TaskType.INTENT_ANALYSIS: AIModel.GEMINI,   # Gemini bon en analyse
            TaskType.CODE_GENERATION: AIModel.OPENAI,   # GPT meilleur en code
            TaskType.DATA_ANALYSIS: AIModel.GEMINI,     # Gemini efficace sur les donnÃ©es
            TaskType.CREATIVE_WRITING: AIModel.OPENAI,  # GPT plus crÃ©atif
            TaskType.TECHNICAL_SUPPORT: AIModel.GEMINI  # Gemini factuel
        }
        
        logger.info("ðŸ§  Service IA hybride initialisÃ©", 
                   strategy=self.strategy,
                   openai_available=self.openai_client is not None,
                   gemini_available=self.gemini_model is not None)
    
    def _initialize_clients(self):
        """Initialisation des clients IA"""
        
        # Client OpenAI
        if settings.OPENAI_API_KEY and settings.OPENAI_API_KEY != "sk-your-openai-key-here":
            try:
                self.openai_client = OpenAI(api_key=settings.OPENAI_API_KEY)
                logger.info("âœ… Client OpenAI initialisÃ©")
            except Exception as e:
                logger.error("âŒ Erreur initialisation OpenAI", error=str(e))
        
        # Client Gemini
        if settings.GOOGLE_AI_KEY and settings.GOOGLE_AI_KEY != "your-gemini-api-key-here":
            try:
                genai.configure(api_key=settings.GOOGLE_AI_KEY)
                self.gemini_model = genai.GenerativeModel(settings.GEMINI_MODEL)
                logger.info("âœ… Client Gemini initialisÃ©")
            except Exception as e:
                logger.error("âŒ Erreur initialisation Gemini", error=str(e))
    
    async def generate_response(self, 
                              prompt: str, 
                              task_type: TaskType = TaskType.CONVERSATION,
                              max_tokens: int = 1000,
                              temperature: float = 0.7,
                              system_prompt: str = None) -> AIResponse:
        """
        GÃ©nÃ©ration de rÃ©ponse avec sÃ©lection intelligente du modÃ¨le
        
        Args:
            prompt: Prompt utilisateur
            task_type: Type de tÃ¢che pour optimiser le choix du modÃ¨le
            max_tokens: Limite de tokens
            temperature: CrÃ©ativitÃ© (0-1)
            system_prompt: Prompt systÃ¨me optionnel
            
        Returns:
            RÃ©ponse IA unifiÃ©e
        """
        
        # SÃ©lection du modÃ¨le selon la stratÃ©gie
        selected_model = self._select_model(task_type)
        
        try:
            if selected_model == AIModel.OPENAI and self.openai_client:
                return await self._generate_openai_response(
                    prompt, max_tokens, temperature, system_prompt
                )
            elif selected_model == AIModel.GEMINI and self.gemini_model:
                return await self._generate_gemini_response(
                    prompt, max_tokens, temperature, system_prompt
                )
            else:
                # Fallback vers le modÃ¨le disponible
                return await self._generate_fallback_response(
                    prompt, max_tokens, temperature, system_prompt
                )
                
        except Exception as e:
            logger.error(f"âŒ Erreur {selected_model}", error=str(e))
            
            # Tentative avec le modÃ¨le de fallback
            fallback_model = AIModel.GEMINI if selected_model == AIModel.OPENAI else AIModel.OPENAI
            
            try:
                if fallback_model == AIModel.OPENAI and self.openai_client:
                    return await self._generate_openai_response(
                        prompt, max_tokens, temperature, system_prompt
                    )
                elif fallback_model == AIModel.GEMINI and self.gemini_model:
                    return await self._generate_gemini_response(
                        prompt, max_tokens, temperature, system_prompt
                    )
            except Exception as fallback_error:
                logger.error("âŒ Erreur fallback", error=str(fallback_error))
                
            # RÃ©ponse d'erreur gracieuse
            return AIResponse(
                content="ðŸ˜… DÃ©solÃ©e, j'ai un problÃ¨me technique temporaire. Pouvez-vous rÃ©essayer ?",
                model=selected_model,
                metadata={"error": str(e)}
            )
    
    def _select_model(self, task_type: TaskType) -> AIModel:
        """SÃ©lection intelligente du modÃ¨le selon la tÃ¢che et la stratÃ©gie"""
        
        if self.strategy == AIStrategy.OPENAI_ONLY:
            return AIModel.OPENAI
        elif self.strategy == AIStrategy.GEMINI_ONLY:
            return AIModel.GEMINI
        elif self.strategy == AIStrategy.HYBRID:
            # SÃ©lection basÃ©e sur la spÃ©cialisation de tÃ¢che
            return self.task_specialization.get(task_type, AIModel.OPENAI)
        elif self.strategy == AIStrategy.FALLBACK:
            # ModÃ¨le principal avec fallback
            primary = AIModel(settings.PRIMARY_MODEL)
            return primary
        
        return AIModel.OPENAI  # DÃ©faut
    
    async def _generate_openai_response(self, 
                                      prompt: str, 
                                      max_tokens: int, 
                                      temperature: float,
                                      system_prompt: str = None) -> AIResponse:
        """GÃ©nÃ©ration de rÃ©ponse via OpenAI GPT"""
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        response = self.openai_client.chat.completions.create(
            model=settings.OPENAI_MODEL,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        return AIResponse(
            content=response.choices[0].message.content,
            model=AIModel.OPENAI,
            metadata={
                "tokens_used": response.usage.total_tokens if response.usage else 0,
                "model": settings.OPENAI_MODEL
            }
        )
    
    async def _generate_gemini_response(self, 
                                      prompt: str, 
                                      max_tokens: int, 
                                      temperature: float,
                                      system_prompt: str = None) -> AIResponse:
        """GÃ©nÃ©ration de rÃ©ponse via Google Gemini"""
        
        # Construction du prompt avec systÃ¨me si fourni
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"{system_prompt}\n\nUtilisateur: {prompt}"
        
        # Configuration de gÃ©nÃ©ration
        generation_config = genai.types.GenerationConfig(
            max_output_tokens=max_tokens,
            temperature=temperature
        )
        
        response = self.gemini_model.generate_content(
            full_prompt,
            generation_config=generation_config
        )
        
        return AIResponse(
            content=response.text,
            model=AIModel.GEMINI,
            metadata={
                "model": settings.GEMINI_MODEL,
                "candidates_count": len(response.candidates) if hasattr(response, 'candidates') else 1
            }
        )
    
    async def _generate_fallback_response(self, 
                                        prompt: str, 
                                        max_tokens: int, 
                                        temperature: float,
                                        system_prompt: str = None) -> AIResponse:
        """GÃ©nÃ©ration avec le modÃ¨le disponible en fallback"""
        
        if self.openai_client:
            return await self._generate_openai_response(prompt, max_tokens, temperature, system_prompt)
        elif self.gemini_model:
            return await self._generate_gemini_response(prompt, max_tokens, temperature, system_prompt)
        else:
            raise Exception("Aucun modÃ¨le IA disponible")
    
    async def compare_responses(self, 
                              prompt: str, 
                              task_type: TaskType = TaskType.CONVERSATION) -> Dict[str, AIResponse]:
        """
        Compare les rÃ©ponses des deux modÃ¨les (pour debug/optimisation)
        
        Returns:
            Dictionnaire avec les rÃ©ponses des deux modÃ¨les
        """
        
        responses = {}
        
        # Test OpenAI
        if self.openai_client:
            try:
                responses["openai"] = await self._generate_openai_response(prompt, 500, 0.7)
            except Exception as e:
                logger.error("âŒ Erreur comparaison OpenAI", error=str(e))
        
        # Test Gemini
        if self.gemini_model:
            try:
                responses["gemini"] = await self._generate_gemini_response(prompt, 500, 0.7)
            except Exception as e:
                logger.error("âŒ Erreur comparaison Gemini", error=str(e))
        
        return responses
    
    def get_service_status(self) -> Dict[str, Any]:
        """Ã‰tat du service IA hybride"""
        return {
            "strategy": self.strategy,
            "models_available": {
                "openai": self.openai_client is not None,
                "gemini": self.gemini_model is not None
            },
            "configuration": {
                "primary_model": settings.PRIMARY_MODEL,
                "fallback_model": settings.FALLBACK_MODEL,
                "openai_model": settings.OPENAI_MODEL,
                "gemini_model": settings.GEMINI_MODEL
            },
            "task_specialization": {k.value: v.value for k, v in self.task_specialization.items()}
        }

# Instance globale du service IA hybride
hybrid_ai = HybridAIService()
